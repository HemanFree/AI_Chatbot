{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HemanFree/AI_Chatbot/blob/main/LLM_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HxojcJbIqTuu"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Retrieve the secret and set it as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"api-key\")\n",
        "\n",
        "api_key = os.environ.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "BxHRBEWB370V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  simple gemini api llm response generator\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Ensure the correct API key name is used, assuming the previous block intended to store the Google API key\n",
        "# If the secret was stored as \"api-key\" but it is a Google API key, the environment variable name should match the API being used.\n",
        "# Let's assume the secret \"api-key\" actually holds your Google API key.\n",
        "GOOGLE_API_KEY = userdata.get(\"api-key\")\n",
        "\n",
        "# Configure the genai library with the API key\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Create a generative model instance\n",
        "model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "# Generate content\n",
        "response = model.generate_content(input(\"Ask LLM: \"))\n",
        "\n",
        "# Print the response\n",
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "2hdmZ_zfq1Gn",
        "outputId": "9ceef49d-3644-451a-9815-ec8d432e991e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask LLM: hello how are you\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am doing well, thank you for asking! How are you today?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make it like a chatbot with a history upto last 10 exchanges\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from collections import deque\n",
        "\n",
        "# Retrieve the secret and set it as an environment variable\n",
        "# Assuming 'api-key' holds the Google API key\n",
        "GOOGLE_API_KEY = userdata.get(\"api-key\")\n",
        "\n",
        "# Configure the genai library with the API key\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Create a generative model instance\n",
        "model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "# Chat history using a deque to maintain the last 10 exchanges\n",
        "chat_history = deque(maxlen=10)\n",
        "\n",
        "def add_to_history(role, content):\n",
        "  chat_history.append({'role': role, 'parts': [content]})\n",
        "\n",
        "def get_history():\n",
        "  return list(chat_history)\n",
        "\n",
        "while True:\n",
        "  user_input = input(\"You: \")\n",
        "  if user_input.lower() == 'quit':\n",
        "    break\n",
        "\n",
        "  # Add user input to history\n",
        "  add_to_history('user', user_input)\n",
        "\n",
        "  # Prepare history for the model, ensuring it's in the correct format\n",
        "  # The first message in the history should be a user message to initiate the conversation context\n",
        "  history_for_model = list(chat_history)\n",
        "  # If the history is not empty and the first item is not a user message, find the first user message\n",
        "  # and start the history from there, or clear the history if no user message is found.\n",
        "  if history_for_model and history_for_model[0]['role'] != 'user':\n",
        "      found_user_message = False\n",
        "      new_history_start_index = 0\n",
        "      for i, message in enumerate(history_for_model):\n",
        "          if message['role'] == 'user':\n",
        "              new_history_start_index = i\n",
        "              found_user_message = True\n",
        "              break\n",
        "      if found_user_message:\n",
        "          history_for_model = history_for_model[new_history_start_index:]\n",
        "      else:\n",
        "          history_for_model = [] # Clear history if no user message is found\n",
        "\n",
        "  try:\n",
        "    # Start the chat with the current history\n",
        "    chat_session = model.start_chat(history=history_for_model)\n",
        "    response = chat_session.send_message(user_input)\n",
        "\n",
        "    # Add model response to history\n",
        "    add_to_history('model', response.text)\n",
        "\n",
        "    # Print the response\n",
        "    print(\"LLM:\", response.text)\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    # Optionally, remove the last user message if the API call failed\n",
        "    if chat_history and chat_history[-1]['role'] == 'user':\n",
        "        chat_history.pop()"
      ],
      "metadata": {
        "id": "l0dq3gZi49qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDDNVLwP7g9l",
        "outputId": "a2802fe1-a920-4d59-c597-437a42db110c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "a\n",
            "1\n",
            "b\n",
            "2\n",
            "c\n"
          ]
        }
      ]
    }
  ]
}